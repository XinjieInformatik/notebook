# 梯度下降 牛顿法推导

## 补充知识
泰勒展开：
如果两个连续的曲线想要一致，则他们在某一点的一阶导数，二，三...n阶导数应该相同。
f(x) = e^x
g(x) = a0 + a1x + a2x^2 + a3x^3 + ... + anx^n
现在用多项式g(x)去拟合f(x)，f(x) = g(x), 则应该在a这个点0-n阶导相同。

![20200606_182218_35](assets/20200606_182218_35.png)

g(x) = f(a) + f'(a)/1!(x-a) + f''(a)/2!(x-a)^2 + ... + fn'/n!(x-a)^n
$$ \sum_i=0^n \frac{f^{(n)}(a)}{n!}(x-a)^n $$

## 多特征线性回归梯度下降
参考： https://www.cnblogs.com/pinard/p/5970503.html
假设使用MSE作为损失函数，m个样本，特征维度n
$$ L(a0,a1,...,an) = 1/(2m) \sum_{j=1}^m(f(x0^j,x1^j,...,xn^j) - y^j)^2 $$
首先初始化a0-an，设置算法终止loss，步长t（学习率）

梯度下降与参数更新流程：
1. 对于ai的梯度，损失函数对ai求偏导, $\frac{\partial}{\partial ai}L(a0,a1,..,an)$
2. 步长乘以梯度，得到参数更新大小
3. 检测对于所有ai的参数更新大小是否都小于终止loss，如果小于训练结束，否则进入步骤4
4. 更新所有的ai
步骤1中ai的梯度计算如下，$1/m \sum_{j=1}^m(f(x0^j,x1^j,...,xn^j) - y^j)x_i^j$

以上是线性归回梯度下降的代数形式，下面给出矩阵描述
f(X)为(m,1)向量，a为(n+1,1)向量，X为(m,n+1)维矩阵，多出来的1是常数项
f(X) = Xa
损失函数的矩阵表达为$L = 1/2 (Xa-Y)^T(Xa-Y)$,其中Y是真值(m,1)维
PS: 记住矩阵的2范数，MSE的矩阵表达为 $1/2 (Xa-Y)^T(Xa-Y)$
对a求偏导为$X^T(Xa-Y)$
因此矩阵形式的梯度更新为: $a = a - tX^T(Xa-Y)$

- 梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。
- 梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。但是每次迭代的时间比梯度下降法长。

## 最小二乘法

## 牛顿法
