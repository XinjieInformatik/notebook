# 决策树再梳理

## GBDT 整理
https://xgboost.readthedocs.io/en/latest/tutorials/model.html

目标函数
$$ obj(\theta) = L(\theta) + \Omega(\theta) $$
其中回归中损失函数L为
$$ L(\theta) = \sum_i (y_i - \hat y_i)^2 $$
二分类中损失函数L为
$$ L(\theta) = -ylogp - (1-y)log(1-p) $$
$$ L(\theta) = \sum_i[y_i ln(1+e^(-\hat y_i) + (1-y_i)ln(1+e^(\hat y_i)))] $$

基于boosting思想，最终对于样本i的打分为$\hat y_i$，其中K为树的棵树
$$ \hat y_i = \sum_{k=1}^K f_k(x_i)$$

目标函数可写为
$$ obj(\theta) = \sum_i^n l(y_i, \hat y_i) + \sum_{k=1}^K \Omega(f_k) $$

基于boosting的残差优化
$$ obj = \sum_{i=1}^n l(y_i, \hat y_i^{(t)}) + \sum_{i=1}^t \Omega(f_i) $$

对于第t棵树上的输出 $\hat y_i^{(t)}$
$$ \hat y_i^{(t)} = \sum_{k=1}^t f_k(x_i) = \hat y_i^{(t-1)} + f_t(x_i) $$

因此对于当前第t棵树的优化目标即为$obj^{(t)}$
$$ obj^{(t)} = \sum_{i=1}^n l(y_i, \hat y_i^{(t-1)} + f_t(x_i)) + \Omega(f_i) $$

用泰勒展开的前两阶去拟合损失函数
$$ obj^{(t)} = \sum_{i=1}^n [l(y_i, \hat y_i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t) $$

$$ g_i = \delta_{\hat} $$
$$ h_i = \delta_{\hat} $$


## lightgbm

https://zhuanlan.zhihu.com/p/38516467

#### XGB 相比 GDBT 优点
- XGB利用了二阶梯度对节点进行划分，比GDBT精度更高
- 损失函数加入了 L1/L2正则项，控制模型复杂度
- 在树节点求不同勾选分裂点时可并行
- Tree shrinkage, column/samples subsampling

#### XGB 缺点
- 需要pre-sorted,消耗内存空间 (data*feature)
- 需要对每个特征依次切分
- 由于pre-sorted, 在寻找特征分裂点时，会产生大量cache随机访问


#### LGB 相比 XGB 优点
- histogram
- GOSS采样
- EFB预处理稀疏数据

##### Gradient-based One-Side Sampling (GOSS)
梯度单边采样。
1. 选取前a%个较大梯度的值作为大梯度值的训练样本
2. 从剩余的1 - a%个较小梯度的值中，我们随机选取其中的b%个作为小梯度值的训练样本
3. 对于较小梯度的样本，也就是b% * #samples，我们在计算信息增益时将其放大(1 - a) / b倍
总的来说就是a% * #samples + b% * #samples个样本作为训练样本。 而这样的构造是为了尽可能保持与总的数据分布一致，并且保证小梯度值的样本得到训练。

![20210826_150256_56](assets/20210826_150256_56.png)

##### Exclusive Feature Bundling
独立特征合并，将若干个特征合并到一起。为了解决数据稀疏问题。
